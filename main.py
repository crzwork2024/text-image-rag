"""
ä¸»åº”ç”¨æ¨¡å— - RAG æ™ºèƒ½é—®ç­”ç³»ç»Ÿ
ä½œè€…ï¼šRAG é¡¹ç›®å›¢é˜Ÿ
æè¿°ï¼šFastAPI åº”ç”¨ä¸»å…¥å£ï¼Œæä¾›é—®ç­” API å’Œ Web ç•Œé¢
"""

import json
import logging
import uuid
from contextlib import asynccontextmanager
from typing import Optional, List
from datetime import datetime, timedelta
import secrets

from fastapi import FastAPI, HTTPException, Depends, Header
from fastapi.staticfiles import StaticFiles
from fastapi.middleware.cors import CORSMiddleware
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from pydantic import BaseModel, Field
import hashlib

from config import config
from core.embeddings import embedding_engine
from core.vector_store import vector_db
from core.llm_client import call_llm
from core.reranker import rerank_engine
from core.query_enhancer import query_enhancer
from core.semantic_cache import SemanticCache
from ingest import run_ingestion
from utils.logger import setup_logger
from utils.responses import QueryResponse, error_response
from utils.exceptions import RetrievalError, LLMAPIError

# åˆå§‹åŒ–æ—¥å¿—
logger = setup_logger(
    "API",
    log_level=config.LOG_LEVEL,
    log_format=config.LOG_FORMAT,
    log_dir=config.LOG_DIR
)

# å…¨å±€å˜é‡ï¼šçˆ¶èŠ‚ç‚¹å­˜å‚¨æ˜ å°„
parent_store = {}

# å…¨å±€å˜é‡ï¼šè¯­ä¹‰ç¼“å­˜
semantic_cache = None

# å…¨å±€å˜é‡ï¼šç®¡ç†å‘˜ä¼šè¯å­˜å‚¨
admin_sessions = {}  # {token: expire_time}

# å®‰å…¨é…ç½®
security = HTTPBearer(auto_error=False)


# ==================== ç®¡ç†å‘˜è®¤è¯è¾…åŠ©å‡½æ•° ====================
def hash_password(password: str) -> str:
    """å“ˆå¸Œå¯†ç """
    return hashlib.sha256(password.encode()).hexdigest()


def verify_admin(username: str, password: str) -> bool:
    """éªŒè¯ç®¡ç†å‘˜å‡­è¯"""
    admin_username = config.ADMIN_USERNAME if hasattr(config, 'ADMIN_USERNAME') else "admin"
    admin_password_hash = config.ADMIN_PASSWORD_HASH if hasattr(config, 'ADMIN_PASSWORD_HASH') else hash_password("admin123")
    
    return username == admin_username and hash_password(password) == admin_password_hash


def generate_admin_token() -> str:
    """ç”Ÿæˆç®¡ç†å‘˜ token"""
    return secrets.token_urlsafe(32)


def verify_admin_token(credentials: Optional[HTTPAuthorizationCredentials] = Depends(security)) -> bool:
    """éªŒè¯ç®¡ç†å‘˜ token"""
    if not credentials:
        raise HTTPException(status_code=401, detail="æœªæä¾›è®¤è¯ä»¤ç‰Œ")
    
    token = credentials.credentials
    if token not in admin_sessions:
        raise HTTPException(status_code=401, detail="è®¤è¯ä»¤ç‰Œæ— æ•ˆæˆ–å·²è¿‡æœŸ")
    
    # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
    if datetime.now() > admin_sessions[token]:
        del admin_sessions[token]
        raise HTTPException(status_code=401, detail="è®¤è¯ä»¤ç‰Œå·²è¿‡æœŸ")
    
    return True


class QueryRequest(BaseModel):
    """æŸ¥è¯¢è¯·æ±‚æ¨¡å‹"""
    prompt: str = Field(..., description="ç”¨æˆ·é—®é¢˜", min_length=1, max_length=1000)
    use_rerank: bool = Field(True, description="æ˜¯å¦ä½¿ç”¨é‡æ’ä¼˜åŒ–")
    use_query_enhancement: bool = Field(False, description="æ˜¯å¦ä½¿ç”¨æŸ¥è¯¢å¢å¼ºï¼ˆHyDEï¼‰")
    session_id: str = Field(default_factory=lambda: str(uuid.uuid4()), description="ä¼šè¯ID")


class CacheConfirmRequest(BaseModel):
    """ç¼“å­˜ç¡®è®¤è¯·æ±‚æ¨¡å‹"""
    confirmation_id: str = Field(..., description="ç¡®è®¤ID")
    user_confirmed: bool = Field(..., description="ç”¨æˆ·æ˜¯å¦ç¡®è®¤ä½¿ç”¨ç¼“å­˜")


class FeedbackRequest(BaseModel):
    """ç”¨æˆ·åé¦ˆè¯·æ±‚æ¨¡å‹"""
    session_id: str = Field(..., description="ä¼šè¯ID")
    question: str = Field(..., description="ç”¨æˆ·é—®é¢˜")
    answer: str = Field(..., description="ç³»ç»Ÿç­”æ¡ˆ")
    satisfied: bool = Field(..., description="ç”¨æˆ·æ˜¯å¦æ»¡æ„")
    source_hashes: Optional[List[str]] = Field(None, description="æºæ–‡æ¡£Hashåˆ—è¡¨")


class AdminLoginRequest(BaseModel):
    """ç®¡ç†å‘˜ç™»å½•è¯·æ±‚æ¨¡å‹"""
    username: str = Field(..., description="ç”¨æˆ·å")
    password: str = Field(..., description="å¯†ç ")


class ManualCacheRequest(BaseModel):
    """æ‰‹åŠ¨æ·»åŠ ç¼“å­˜è¯·æ±‚æ¨¡å‹"""
    question: str = Field(..., description="é—®é¢˜æ–‡æœ¬", min_length=1)
    answer: str = Field(..., description="ç­”æ¡ˆæ–‡æœ¬", min_length=1)
    quality_score: int = Field(10, description="è´¨é‡åˆ†æ•°", ge=0, le=10)
    source_info: Optional[str] = Field(None, description="æºæ–‡ä»¶ä¿¡æ¯ï¼ˆç®¡ç†å‘˜æ‰‹åŠ¨å¡«å†™ï¼‰")


class ClearCacheRequest(BaseModel):
    """æ¸…é™¤ç¼“å­˜è¯·æ±‚æ¨¡å‹"""
    cache_types: Optional[List[str]] = Field(None, description="è¦æ¸…é™¤çš„ç¼“å­˜ç±»å‹")
    confirm: bool = Field(False, description="ç¡®è®¤æ¸…é™¤")

@asynccontextmanager
async def lifespan(app: FastAPI):
    """
    åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†

    å¯åŠ¨æ—¶:
    - åŠ è½½çˆ¶èŠ‚ç‚¹æ˜ å°„
    - æ£€æŸ¥å‘é‡æ•°æ®åº“ï¼Œå¦‚æœä¸ºç©ºåˆ™è‡ªåŠ¨æ‰§è¡Œæ‘„å–

    å…³é—­æ—¶:
    - æ¸…ç†èµ„æº
    """
    global parent_store, semantic_cache

    logger.info("=" * 60)
    logger.info("ç³»ç»Ÿå¯åŠ¨ä¸­...")
    logger.info("=" * 60)

    try:
        # åˆ›å»ºå¿…è¦çš„ç›®å½•
        config.create_directories()

        # åˆå§‹åŒ–è¯­ä¹‰ç¼“å­˜
        logger.info("åˆå§‹åŒ–è¯­ä¹‰ç¼“å­˜...")
        semantic_cache = SemanticCache(embedding_engine)
        if semantic_cache.is_available():
            logger.info("âœ“ è¯­ä¹‰ç¼“å­˜å·²å¯ç”¨")
        else:
            logger.warning("âš ï¸ è¯­ä¹‰ç¼“å­˜ä¸å¯ç”¨ï¼ˆRedisè¿æ¥å¤±è´¥ï¼‰ï¼Œå°†è·³è¿‡ç¼“å­˜åŠŸèƒ½")

        # åŠ è½½çˆ¶èŠ‚ç‚¹æ˜ å°„
        if config.PARENT_STORE_PATH.exists():
            logger.info(f"åŠ è½½çˆ¶èŠ‚ç‚¹æ˜ å°„: {config.PARENT_STORE_PATH}")
            with open(config.PARENT_STORE_PATH, "r", encoding="utf-8") as f:
                parent_store = json.load(f)
            logger.info(f"âœ“ åŠ è½½ {len(parent_store)} ä¸ªçˆ¶èŠ‚ç‚¹")
        else:
            logger.warning("çˆ¶èŠ‚ç‚¹æ˜ å°„æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†åœ¨é¦–æ¬¡æ‘„å–æ—¶åˆ›å»º")

        # æ£€æŸ¥å‘é‡æ•°æ®åº“
        doc_count = vector_db.count()
        logger.info(f"å‘é‡æ•°æ®åº“æ–‡æ¡£æ•°é‡: {doc_count}")

        if doc_count == 0:
            logger.warning("å‘é‡æ•°æ®åº“ä¸ºç©ºï¼Œå¼€å§‹è‡ªåŠ¨æ‘„å–...")
            run_ingestion()

            # é‡æ–°åŠ è½½çˆ¶èŠ‚ç‚¹æ˜ å°„
            with open(config.PARENT_STORE_PATH, "r", encoding="utf-8") as f:
                parent_store = json.load(f)

            logger.info("âœ“ è‡ªåŠ¨æ‘„å–å®Œæˆ")

        logger.info("=" * 60)
        logger.info("ç³»ç»Ÿå¯åŠ¨å®Œæˆï¼ŒæœåŠ¡å°±ç»ª")
        logger.info("=" * 60)

        yield

    except Exception as e:
        logger.error(f"ç³»ç»Ÿå¯åŠ¨å¤±è´¥: {e}")
        raise

    finally:
        logger.info("=" * 60)
        logger.info("ç³»ç»Ÿå…³é—­")
        logger.info("=" * 60)


# åˆ›å»º FastAPI åº”ç”¨
app = FastAPI(
    title="RAG æ™ºèƒ½é—®ç­”ç³»ç»Ÿ API",
    description="åŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆçš„æ™ºèƒ½é—®ç­”æœåŠ¡",
    version="1.0.0",
    lifespan=lifespan
)

# æ·»åŠ  CORS ä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.post("/query", response_model=QueryResponse)
async def query_rag(req: QueryRequest):
    """
    RAG é—®ç­”æ¥å£

    æµç¨‹:
    1. å‘é‡æ£€ç´¢ï¼šä»å‘é‡æ•°æ®åº“å¬å›å€™é€‰æ–‡æ¡£
    2. åˆæ­¥è¿‡æ»¤ï¼šæ ¹æ®ç›¸ä¼¼åº¦é˜ˆå€¼è¿‡æ»¤
    3. é‡æ’ï¼ˆå¯é€‰ï¼‰ï¼šä½¿ç”¨ Rerank æ¨¡å‹ç²¾ç¡®æ’åº
    4. ä¸Šä¸‹æ–‡ç»„è£…ï¼šæ ¹æ®çˆ¶èŠ‚ç‚¹å“ˆå¸Œè·å–å®Œæ•´ç« èŠ‚
    5. LLM ç”Ÿæˆï¼šè°ƒç”¨å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆå›ç­”

    å‚æ•°:
        req: æŸ¥è¯¢è¯·æ±‚ï¼ŒåŒ…å«é—®é¢˜å’Œæ˜¯å¦ä½¿ç”¨é‡æ’

    è¿”å›:
        QueryResponse: åŒ…å«ç­”æ¡ˆã€æœ€é«˜åˆ†æ•°ã€æ¥æºæ•°é‡
    """
    # æ„å»ºæ¨¡å¼æ ‡è¯†
    modes = []
    if req.use_query_enhancement:
        modes.append("æŸ¥è¯¢å¢å¼º")
    modes.append("ç²¾æ’æ¨¡å¼" if req.use_rerank else "ç›´å–æ¨¡å¼")
    mode_str = " + ".join(modes)

    logger.info("=" * 60)
    logger.info(f"[æ–°æŸ¥è¯¢] {mode_str}")
    logger.info(f"é—®é¢˜: {req.prompt[:100]}{'...' if len(req.prompt) > 100 else ''}")
    logger.info("=" * 60)

    try:
        # ==================== æ­¥éª¤ 0: è¯­ä¹‰ç¼“å­˜æŸ¥è¯¢ ====================
        if semantic_cache and semantic_cache.is_available():
            logger.info("æ­¥éª¤ 0: æŸ¥è¯¢è¯­ä¹‰ç¼“å­˜")
            cache_result = await semantic_cache.query(req.prompt, req.session_id)

            if cache_result["status"] == "hit":
                # ç¼“å­˜ç›´æ¥å‘½ä¸­
                logger.info("âš¡ ç¼“å­˜ç›´æ¥å‘½ä¸­ï¼Œè¿”å›ç¼“å­˜ç­”æ¡ˆ")
                return QueryResponse(
                    answer=cache_result["answer"],
                    best_score=f"{cache_result['similarity']:.2%}",
                    sources_count=0,
                    metadata={
                        "from_cache": True,
                        "cache_type": "direct_hit",
                        "cached_question": cache_result["cached_question"],
                        "similarity": f"{cache_result['similarity']:.2%}"
                    }
                )

            elif cache_result["status"] == "pending_confirm":
                # éœ€è¦ç”¨æˆ·ç¡®è®¤
                logger.info("â¸ï¸ å‘ç°ç›¸ä¼¼é—®é¢˜ï¼Œç­‰å¾…ç”¨æˆ·ç¡®è®¤")
                return {
                    "need_confirmation": True,
                    "cached_question": cache_result["cached_question"],
                    "similarity": f"{cache_result['similarity']:.2%}",
                    "confirmation_id": cache_result["confirmation_id"],
                    "message": "å‘ç°ç›¸ä¼¼é—®é¢˜ï¼Œæ˜¯å¦ä½¿ç”¨ç¼“å­˜ç­”æ¡ˆï¼Ÿ"
                }

            # cache_result["status"] == "miss" â†’ ç»§ç»­æ­£å¸¸æµç¨‹
            logger.info("ğŸ”„ ç¼“å­˜æœªå‘½ä¸­ï¼Œæ‰§è¡Œå®Œæ•´æ£€ç´¢æµç¨‹")

        # ==================== æ­¥éª¤ 1: æŸ¥è¯¢å¢å¼ºï¼ˆå¯é€‰ï¼‰====================
        enhanced_query = None
        if req.use_query_enhancement and query_enhancer.is_available():
            logger.info("æ­¥éª¤ 1/6: æŸ¥è¯¢å¢å¼ºï¼ˆç”Ÿæˆå‡è®¾å…³é”®è¯ï¼‰")
            enhanced_query = query_enhancer.generate_hypothetical_keywords(req.prompt)

            if enhanced_query:
                logger.info(f"âœ“ ç”Ÿæˆå…³é”®è¯: {enhanced_query[:100]}...")
            else:
                logger.warning("âœ— æŸ¥è¯¢å¢å¼ºå¤±è´¥ï¼Œé™çº§ä¸ºæ™®é€šæ£€ç´¢")
                req.use_query_enhancement = False

        # ==================== æ­¥éª¤ 2: å‘é‡æ£€ç´¢ ====================
        step_num = "2/6" if req.use_query_enhancement else "1/5"
        logger.info(f"æ­¥éª¤ {step_num}: å‘é‡æ£€ç´¢ (å¬å›æ•°: {config.RETRIEVAL_COUNT})")

        # åŸå§‹é—®é¢˜æ£€ç´¢
        query_vec = embedding_engine.encode([req.prompt])
        results_query = vector_db.query(query_vec, n_results=config.RETRIEVAL_COUNT)

        raw_docs = results_query["documents"][0]
        raw_metas = results_query["metadatas"][0]
        raw_dists = results_query["distances"][0]
        raw_ids = results_query["ids"][0]

        logger.info(f"âœ“ åŸé—®é¢˜å¬å› {len(raw_docs)} ä¸ªå€™é€‰æ–‡æ¡£")

        # å¦‚æœå¯ç”¨æŸ¥è¯¢å¢å¼ºï¼Œæ‰§è¡Œç¬¬äºŒæ¬¡æ£€ç´¢
        if req.use_query_enhancement and enhanced_query:
            logger.info(f"æ­¥éª¤ 2.5/6: ä½¿ç”¨å…³é”®è¯è¿›è¡ŒäºŒæ¬¡æ£€ç´¢")

            enhanced_vec = embedding_engine.encode([enhanced_query])
            results_enhanced = vector_db.query(enhanced_vec, n_results=config.RETRIEVAL_COUNT)

            enhanced_docs = results_enhanced["documents"][0]
            enhanced_metas = results_enhanced["metadatas"][0]
            enhanced_dists = results_enhanced["distances"][0]
            enhanced_ids = results_enhanced["ids"][0]

            logger.info(f"âœ“ å…³é”®è¯å¬å› {len(enhanced_docs)} ä¸ªå€™é€‰æ–‡æ¡£")

            # èåˆä¸¤æ¬¡æ£€ç´¢ç»“æœ - ä½¿ç”¨åŠ æƒå¹³å‡
            logger.info("èåˆä¸¤æ¬¡æ£€ç´¢ç»“æœï¼ˆåŠ æƒèåˆï¼‰...")

            # æ„å»ºIDåˆ°åˆ†æ•°çš„æ˜ å°„
            query_scores = {}
            enhanced_scores = {}

            for i, doc_id in enumerate(raw_ids):
                sim = 1 - raw_dists[i]
                query_scores[doc_id] = {
                    'similarity': sim,
                    'doc': raw_docs[i],
                    'meta': raw_metas[i]
                }

            for i, doc_id in enumerate(enhanced_ids):
                sim = 1 - enhanced_dists[i]
                enhanced_scores[doc_id] = {
                    'similarity': sim,
                    'doc': enhanced_docs[i],
                    'meta': enhanced_metas[i]
                }

            # åˆå¹¶å¹¶åŠ æƒ
            all_doc_ids = set(query_scores.keys()) | set(enhanced_scores.keys())
            merged_results = []

            query_weight = config.QUERY_ENHANCEMENT_WEIGHT
            enhanced_weight = 1 - query_weight

            for doc_id in all_doc_ids:
                q_sim = query_scores.get(doc_id, {}).get('similarity', 0)
                e_sim = enhanced_scores.get(doc_id, {}).get('similarity', 0)

                # åŠ æƒèåˆ
                final_sim = query_weight * q_sim + enhanced_weight * e_sim

                # ä½¿ç”¨åŸé—®é¢˜çš„æ–‡æ¡£å†…å®¹ï¼ˆä¼˜å…ˆï¼‰
                doc_content = query_scores.get(doc_id, {}).get('doc') or enhanced_scores.get(doc_id, {}).get('doc')
                doc_meta = query_scores.get(doc_id, {}).get('meta') or enhanced_scores.get(doc_id, {}).get('meta')

                merged_results.append({
                    'id': doc_id,
                    'similarity': final_sim,
                    'distance': 1 - final_sim,
                    'doc': doc_content,
                    'meta': doc_meta
                })

            # æŒ‰èåˆåçš„ç›¸ä¼¼åº¦æ’åº
            merged_results.sort(key=lambda x: x['similarity'], reverse=True)

            # é‡æ–°ç»„ç»‡ä¸ºåŸæ ¼å¼ï¼Œä¿ç•™å‰10ä¸ª
            raw_docs = [r['doc'] for r in merged_results[:10]]
            raw_metas = [r['meta'] for r in merged_results[:10]]
            raw_dists = [r['distance'] for r in merged_results[:10]]

            logger.info(f"âœ“ èåˆå®Œæˆï¼Œä¿ç•™å‰ 10 ä¸ªç»“æœ")

            # æ˜¾ç¤ºèåˆåçš„åˆ†æ•°
            logger.info("èåˆåçš„å‰10ä¸ªç»“æœ:")
            for i, r in enumerate(merged_results[:10], 1):
                h = r['meta'].get('parent_hash', 'N/A')
                logger.info(f"  [{i:2d}] èåˆåˆ†æ•°: {r['similarity']*100:>6.2f}% | çˆ¶Hash: {h[:16]}...")

        # ==================== æ­¥éª¤ 3: å‘é‡åˆç­› ====================
        step_num = "3/6" if req.use_query_enhancement else "2/5"

        # æ ¹æ®æ˜¯å¦ä½¿ç”¨ç²¾æ’é€‰æ‹©ä¸åŒçš„é˜ˆå€¼
        if req.use_rerank and rerank_engine.is_available():
            threshold = config.VECTOR_SEARCH_THRESHOLD_WITH_RERANK
            threshold_mode = "å®½æ¾(ç²¾æ’æ¨¡å¼)"
        else:
            threshold = config.VECTOR_SEARCH_THRESHOLD_WITHOUT_RERANK
            threshold_mode = "ä¸¥æ ¼(ç›´å–æ¨¡å¼)"

        logger.info(f"æ­¥éª¤ {step_num}: å‘é‡åˆç­› (é˜ˆå€¼: {threshold} - {threshold_mode})")

        # æ˜¾ç¤ºå‰10ä¸ªå€™é€‰çš„ç›¸ä¼¼åº¦åˆ†æ•°å’Œçˆ¶Hash
        logger.info("=" * 60)
        logger.info("ã€å‘é‡æ£€ç´¢ã€‘å‰10ä¸ªå€™é€‰ï¼ˆæŒ‰ç›¸ä¼¼åº¦æ’åºï¼‰:")
        for i in range(min(10, len(raw_docs))):
            sim = 1 - raw_dists[i]
            sim_pct = f"{round(sim * 100, 2)}%"
            h = raw_metas[i].get("parent_hash", "N/A")
            pass_mark = "âœ“" if sim >= threshold else "âœ—"
            logger.info(f"  [{pass_mark}] [{i+1:2d}] ç›¸ä¼¼åº¦: {sim_pct:>7s} | çˆ¶Hash: {h[:16]}...")
        logger.info("=" * 60)

        candidates = []
        candidates_meta = []

        for i in range(len(raw_docs)):
            sim = 1 - raw_dists[i]
            h = raw_metas[i].get("parent_hash", "N/A")

            if sim >= threshold:
                candidates.append(raw_docs[i])
                candidates_meta.append(raw_metas[i])

        logger.info(f"âœ“ ç­›é€‰åå‰©ä½™ {len(candidates)} ä¸ªå€™é€‰æ–‡æ¡£")

        # å¦‚æœæ²¡æœ‰å€™é€‰æ–‡æ¡£ï¼Œç›´æ¥è¿”å›
        if not candidates:
            logger.warning("æœªæ‰¾åˆ°ç›¸å…³å†…å®¹")
            return QueryResponse(
                answer="æŠ±æ­‰ï¼Œæœªæ‰¾åˆ°ä¸æ‚¨é—®é¢˜ç›¸å…³çš„å†…å®¹ã€‚",
                best_score="0%",
                sources_count=0
            )

        final_hashes = []
        score_summaries = []

        # ==================== æ­¥éª¤ 4: é‡æ’ï¼ˆå¯é€‰ï¼‰====================
        if req.use_rerank and rerank_engine.is_available():
            step_num = "4/6" if req.use_query_enhancement else "3/5"
            logger.info(f"æ­¥éª¤ {step_num}: æ‰§è¡Œç²¾æ’ (å€™é€‰æ•°: {len(candidates)})")

            rerank_data = rerank_engine.rerank(req.prompt, candidates)

            if rerank_data:
                # æ˜¾ç¤ºæ‰€æœ‰ç²¾æ’ç»“æœ
                logger.info("=" * 60)
                logger.info("ã€ç²¾æ’ç»“æœã€‘æŒ‰ç›¸å…³åº¦æ’åº:")

                for i, res in enumerate(rerank_data):
                    orig_idx = res["index"]
                    score = res["relevance_score"]
                    p_hash = candidates_meta[orig_idx].get("parent_hash", "N/A")

                    score_pct = f"{round(score * 100, 2)}%"

                    # æ ¹æ®é˜ˆå€¼è¿‡æ»¤
                    if score >= config.RERANK_THRESHOLD:
                        logger.info(f"  [âœ“ {i+1:2d}] ç²¾æ’åˆ†æ•°: {score_pct:>7s} | çˆ¶Hash: {p_hash[:16]}... (å·²é€‰å…¥)")
                        final_hashes.append(p_hash)
                    else:
                        logger.info(f"  [âœ— {i+1:2d}] ç²¾æ’åˆ†æ•°: {score_pct:>7s} | çˆ¶Hash: {p_hash[:16]}... (æœªè¾¾é˜ˆå€¼)")

                    score_summaries.append({"rank": i+1, "rerank_score": score_pct})

                logger.info("=" * 60)
                logger.info(f"âœ“ ç²¾æ’å®Œæˆï¼Œä¿ç•™ {len(final_hashes)} ä¸ªé«˜ç›¸å…³æ–‡æ¡£")
            else:
                logger.warning("ç²¾æ’å¤±è´¥ï¼Œé™çº§ä¸ºç›´å–æ¨¡å¼")
                req.use_rerank = False

        # ==================== æ­¥éª¤ 4: ç›´å–æ¨¡å¼ï¼ˆå¤‡é€‰ï¼‰====================
        if not req.use_rerank or not rerank_engine.is_available():
            step_num = "4/6" if req.use_query_enhancement else "3/5"
            logger.info(f"æ­¥éª¤ {step_num}: ç›´å–æ¨¡å¼ï¼Œé€‰æ‹©å‰ {config.RERANK_TOP_K} å")

            # æ˜¾ç¤ºç›´å–çš„ç»“æœ
            logger.info("=" * 60)
            logger.info("ã€ç›´å–æ¨¡å¼ã€‘æŒ‰å‘é‡ç›¸ä¼¼åº¦æ’åº:")

            for i in range(min(config.RERANK_TOP_K, len(candidates_meta))):
                p_hash = candidates_meta[i].get("parent_hash", "N/A")
                # ç›´å–æ¨¡å¼éœ€è¦ä»candidatesä¸­è·å–ï¼Œå› ä¸ºå·²ç»è¿‡æ»¤äº†
                # æ‰¾åˆ°è¿™ä¸ªcandidateåœ¨åŸå§‹åˆ—è¡¨ä¸­çš„ä½ç½®
                candidate_idx = i
                for j in range(len(raw_docs)):
                    if raw_metas[j].get("parent_hash") == p_hash:
                        candidate_idx = j
                        break

                sim = 1 - raw_dists[candidate_idx]
                score_pct = f"{round(sim * 100, 2)}%"
                
                # ç›´å–æ¨¡å¼ä½¿ç”¨ä¸¥æ ¼é˜ˆå€¼ï¼ˆé»˜è®¤50%ï¼‰
                direct_threshold = config.VECTOR_SEARCH_THRESHOLD_WITHOUT_RERANK
                
                if sim >= direct_threshold:
                    logger.info(f"  [âœ“ {i+1}] ç›¸ä¼¼åº¦: {score_pct:>7s} | çˆ¶Hash: {p_hash[:16]}... (å·²é€‰å…¥)")
                    final_hashes.append(p_hash)
                    score_summaries.append({"rank": i+1, "rerank_score": score_pct})
                else:
                    logger.info(f"  [âœ— {i+1}] ç›¸ä¼¼åº¦: {score_pct:>7s} | çˆ¶Hash: {p_hash[:16]}... (ä½äºé˜ˆå€¼{direct_threshold*100:.0f}%ï¼Œå·²è¿‡æ»¤)")
                    score_summaries.append({"rank": i+1, "rerank_score": score_pct})

            logger.info("=" * 60)
            logger.info(f"âœ“ ç›´å– {len(final_hashes)} ä¸ªæ–‡æ¡£")

        # ==================== æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆç»“æœ ====================
        if not final_hashes:
            logger.warning("æ‰€æœ‰ç»“æœå‡ä½äºç›¸å…³åº¦é˜ˆå€¼ï¼Œæœªæ‰¾åˆ°è¶³å¤Ÿç›¸å…³çš„å†…å®¹")
            return QueryResponse(
                answer="æŠ±æ­‰ï¼Œæœªæ‰¾åˆ°ä¸æ‚¨é—®é¢˜è¶³å¤Ÿç›¸å…³çš„å†…å®¹ã€‚\n\nå»ºè®®ï¼š\n1. å°è¯•é‡æ–°è¡¨è¿°é—®é¢˜\n2. ä½¿ç”¨ä¸åŒçš„å…³é”®è¯\n3. æ£€æŸ¥çŸ¥è¯†åº“æ˜¯å¦åŒ…å«ç›¸å…³ä¿¡æ¯",
                best_score=score_summaries[0]["rerank_score"] if score_summaries else "0%",
                sources_count=0
            )

        # ==================== æ­¥éª¤ 5: è·å–å®Œæ•´ä¸Šä¸‹æ–‡ ====================
        step_num = "5/6" if req.use_query_enhancement else "4/5"
        logger.info(f"æ­¥éª¤ {step_num}: ç»„è£…ä¸Šä¸‹æ–‡")

        unique_hashes = list(dict.fromkeys(final_hashes))

        # è®°å½•æ‰€æœ‰æœ€ç»ˆé€‰å®šçš„çˆ¶hash
        logger.info(f"æœ€ç»ˆé€‰å®šçš„çˆ¶Hashåˆ—è¡¨ (å…± {len(unique_hashes)} ä¸ª):")
        for idx, h in enumerate(unique_hashes, 1):
            logger.info(f"  [{idx}] Hash: {h}")

        retrieved_sections = [
            parent_store.get(h) for h in unique_hashes
            if parent_store.get(h)
        ]

        logger.info(f"âœ“ æå– {len(retrieved_sections)} ä¸ªå®Œæ•´ç« èŠ‚")

        if not retrieved_sections:
            logger.warning("æœªèƒ½è·å–æœ‰æ•ˆä¸Šä¸‹æ–‡")
            return QueryResponse(
                answer="æŠ±æ­‰ï¼Œæœªèƒ½æ‰¾åˆ°è¶³å¤Ÿç›¸å…³çš„å†…å®¹ã€‚",
                best_score=score_summaries[0]["rerank_score"] if score_summaries else "0%",
                sources_count=0
            )

        # ==================== æ­¥éª¤ 6: LLM ç”Ÿæˆå›ç­” ====================
        step_num = "6/6" if req.use_query_enhancement else "5/5"
        logger.info(f"æ­¥éª¤ {step_num}: LLM ç”Ÿæˆå›ç­”")

        context = "\n\n---\n\n".join(retrieved_sections)
        logger.info(f"ä¸Šä¸‹æ–‡æ€»é•¿åº¦: {len(context)} å­—ç¬¦")
        logger.info(f"ä¼ ç»™LLMçš„çˆ¶èŠ‚ç‚¹æ•°é‡: {len(retrieved_sections)}")

        try:
            answer = call_llm(context, req.prompt)
            logger.info(f"âœ“ å›ç­”ç”ŸæˆæˆåŠŸï¼Œé•¿åº¦: {len(answer)} å­—ç¬¦")
        except LLMAPIError as e:
            logger.error(f"LLM ç”Ÿæˆå¤±è´¥: {e.message}")
            answer = "æŠ±æ­‰ï¼ŒAI ç”Ÿæˆå›ç­”æ—¶å‡ºç°é”™è¯¯ï¼Œè¯·ç¨åé‡è¯•ã€‚"

        # ==================== æ·»åŠ åˆ°ç¼“å­˜ ====================
        # æ³¨æ„ï¼šç›®å‰è‡ªåŠ¨ç¼“å­˜å·²ç¦ç”¨ï¼Œåªæœ‰ç”¨æˆ·ç¡®è®¤æ»¡æ„æˆ–ç®¡ç†å‘˜æ‰‹åŠ¨æ·»åŠ æ‰ä¼šç¼“å­˜
        # if semantic_cache and semantic_cache.is_available():
        #     logger.info("ğŸ’¾ æ·»åŠ ç­”æ¡ˆåˆ°è¯­ä¹‰ç¼“å­˜")
        #     semantic_cache.set(req.prompt, answer)

        # ==================== è¿”å›ç»“æœ ====================
        logger.info("=" * 60)
        logger.info("[æŸ¥è¯¢å®Œæˆ]")
        logger.info("=" * 60)

        return QueryResponse(
            answer=answer,
            best_score=score_summaries[0]["rerank_score"] if score_summaries else "0%",
            sources_count=len(retrieved_sections),
            source_hashes=unique_hashes  # è¿”å›æºæ–‡æ¡£çš„ parent_hash åˆ—è¡¨
        )

    except Exception as e:
        logger.error(f"æŸ¥è¯¢å¤„ç†å¼‚å¸¸: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=error_response(
                error="æŸ¥è¯¢å¤„ç†å¤±è´¥",
                details=str(e),
                code="QUERY_ERROR"
            )
        )

@app.post("/cache/confirm")
async def confirm_cache(req: CacheConfirmRequest):
    """
    å¤„ç†ç”¨æˆ·çš„ç¼“å­˜ç¡®è®¤

    å‚æ•°:
        req: ç¡®è®¤è¯·æ±‚ï¼ŒåŒ…å«ç¡®è®¤IDå’Œç”¨æˆ·å†³å®š

    è¿”å›:
        å¦‚æœç”¨æˆ·ç¡®è®¤ï¼Œè¿”å›ç¼“å­˜ç­”æ¡ˆï¼›å¦åˆ™æç¤ºé‡æ–°æŸ¥è¯¢
    """
    if not semantic_cache or not semantic_cache.is_available():
        raise HTTPException(status_code=503, detail="ç¼“å­˜æœåŠ¡ä¸å¯ç”¨")

    cached_answer = await semantic_cache.confirm_cache(
        req.confirmation_id,
        req.user_confirmed
    )

    if req.user_confirmed and cached_answer:
        # ç”¨æˆ·ç¡®è®¤ä½¿ç”¨ç¼“å­˜
        return {
            "answer": cached_answer,
            "from_cache": True,
            "best_score": "95%+",
            "sources_count": 0,
            "message": "å·²ä½¿ç”¨ç¼“å­˜ç­”æ¡ˆ"
        }
    else:
        # ç”¨æˆ·æ‹’ç»æˆ–ç¡®è®¤IDè¿‡æœŸ â†’ éœ€è¦å‰ç«¯é‡æ–°å‘èµ·æŸ¥è¯¢
        return {
            "need_requery": True,
            "message": "è¯·é‡æ–°æé—®ä»¥è·å–æ–°ç­”æ¡ˆ"
        }


@app.post("/cache/feedback")
async def cache_feedback(req: FeedbackRequest):
    """
    ç”¨æˆ·æ»¡æ„åº¦åé¦ˆ

    å½“ç”¨æˆ·å¯¹ç­”æ¡ˆæ»¡æ„æ—¶ï¼Œå°†é—®ç­”å¯¹æ·»åŠ åˆ°é«˜è´¨é‡ç¼“å­˜
    """
    if not semantic_cache or not semantic_cache.is_available():
        return {"status": "unavailable", "message": "ç¼“å­˜æœåŠ¡ä¸å¯ç”¨"}

    try:
        if req.satisfied:
            # ç”¨æˆ·æ»¡æ„ï¼Œæ·»åŠ åˆ°é«˜è´¨é‡ç¼“å­˜
            # å°† source_hashes è½¬æ¢ä¸º JSON å­—ç¬¦ä¸²
            import json
            source_info = json.dumps(req.source_hashes) if req.source_hashes else None
            
            semantic_cache.set(
                req.question,
                req.answer,
                cache_type="confirmed",
                quality_score=5,
                source_info=source_info
            )
            logger.info(f"âœ… ç”¨æˆ·åé¦ˆæ»¡æ„ï¼Œå·²æ·»åŠ åˆ°é«˜è´¨é‡ç¼“å­˜: {req.question[:50]}")
            return {
                "status": "success",
                "message": "æ„Ÿè°¢åé¦ˆï¼å·²ä¿å­˜åˆ°ç²¾é€‰é—®ç­”"
            }
        else:
            # ç”¨æˆ·ä¸æ»¡æ„ï¼Œè®°å½•ä½†ä¸ç¼“å­˜
            logger.info(f"âŒ ç”¨æˆ·åé¦ˆä¸æ»¡æ„: {req.question[:50]}")
            return {
                "status": "success",
                "message": "æ„Ÿè°¢åé¦ˆï¼æˆ‘ä»¬ä¼šæ”¹è¿›"
            }
    except Exception as e:
        logger.error(f"å¤„ç†ç”¨æˆ·åé¦ˆæ—¶å‡ºé”™: {e}")
        raise HTTPException(status_code=500, detail="å¤„ç†åé¦ˆå¤±è´¥")


@app.get("/cache/popular")
async def get_popular_questions():
    """
    è·å–çƒ­é—¨é—®é¢˜ï¼ˆä¾›å‰ç«¯æ˜¾ç¤ºï¼‰

    è¿”å›:
        çƒ­é—¨é—®é¢˜åˆ—è¡¨ï¼ŒåŒ…å«é—®é¢˜ã€è®¿é—®æ¬¡æ•°ã€ç›¸ä¼¼é—®é¢˜æ•°
    """
    if not semantic_cache or not semantic_cache.is_available():
        return {"popular_questions": []}

    popular_questions = semantic_cache.get_popular_questions(3)
    return {"popular_questions": popular_questions}


@app.get("/cache/stats")
async def get_cache_stats():
    """
    è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯

    è¿”å›:
        ç¼“å­˜ç»Ÿè®¡æ•°æ®ï¼ŒåŒ…å«ç¼“å­˜æ¡ç›®æ•°ã€å‘½ä¸­æ¬¡æ•°ç­‰
    """
    if not semantic_cache or not semantic_cache.is_available():
        return {
            "available": False,
            "message": "ç¼“å­˜æœåŠ¡ä¸å¯ç”¨"
        }

    stats = semantic_cache.get_cache_stats()
    return stats


@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    cache_available = semantic_cache and semantic_cache.is_available()
    return {
        "status": "healthy",
        "vector_db_docs": vector_db.count(),
        "parent_store_size": len(parent_store),
        "cache_available": cache_available
    }


# ==================== ç®¡ç†å‘˜ API ====================

@app.post("/admin/login")
async def admin_login(req: AdminLoginRequest):
    """
    ç®¡ç†å‘˜ç™»å½•

    è¿”å›:
        è®¤è¯ä»¤ç‰Œå’Œè¿‡æœŸæ—¶é—´
    """
    if not verify_admin(req.username, req.password):
        logger.warning(f"ç®¡ç†å‘˜ç™»å½•å¤±è´¥: {req.username}")
        raise HTTPException(status_code=401, detail="ç”¨æˆ·åæˆ–å¯†ç é”™è¯¯")

    # ç”Ÿæˆ token
    token = generate_admin_token()
    expire_time = datetime.now() + timedelta(hours=1)
    admin_sessions[token] = expire_time

    logger.info(f"âœ… ç®¡ç†å‘˜ç™»å½•æˆåŠŸ: {req.username}")
    return {
        "token": token,
        "expires_in": 3600,
        "expires_at": expire_time.isoformat()
    }


@app.post("/admin/logout")
async def admin_logout(authorized: bool = Depends(verify_admin_token),
                       credentials: HTTPAuthorizationCredentials = Depends(security)):
    """ç®¡ç†å‘˜ç™»å‡º"""
    token = credentials.credentials
    if token in admin_sessions:
        del admin_sessions[token]
        logger.info("âœ… ç®¡ç†å‘˜ç™»å‡ºæˆåŠŸ")
    
    return {"status": "success", "message": "å·²ç™»å‡º"}


@app.get("/admin/hot-questions")
async def get_hot_questions(
    limit: int = 50,
    min_count: int = 1,
    authorized: bool = Depends(verify_admin_token)
):
    """
    è·å–çƒ­é—¨é—®é¢˜åˆ—è¡¨ï¼ˆç®¡ç†å‘˜ï¼‰

    å‚æ•°:
        limit: è¿”å›æ•°é‡
        min_count: æœ€å°æé—®æ¬¡æ•°

    è¿”å›:
        çƒ­é—¨é—®é¢˜åˆ—è¡¨
    """
    if not semantic_cache or not semantic_cache.is_available():
        return {"hot_questions": []}

    try:
        # è·å–æ‰€æœ‰çƒ­é—¨é—®é¢˜
        popular = semantic_cache.redis.zrevrange("cache:popular", 0, limit - 1, withscores=True)
        
        result = []
        for question_bytes, count in popular:
            if count < min_count:
                continue
                
            question = question_bytes.decode('utf-8') if isinstance(question_bytes, bytes) else question_bytes
            cache_id = semantic_cache._compute_hash(question)
            
            # æ£€æŸ¥æ˜¯å¦å·²ç¼“å­˜
            cached_data = semantic_cache.redis.hgetall(f"cache:question:{cache_id}")
            is_cached = bool(cached_data)
            cache_type = None
            
            if is_cached:
                cache_type = cached_data.get(b'cache_type', b'auto').decode('utf-8')
            
            result.append({
                "question": question,
                "count": int(count),
                "cached": is_cached,
                "cache_type": cache_type,
                "cache_id": cache_id
            })
        
        return {"hot_questions": result}
        
    except Exception as e:
        logger.error(f"è·å–çƒ­é—¨é—®é¢˜æ—¶å‡ºé”™: {e}")
        raise HTTPException(status_code=500, detail="è·å–çƒ­é—¨é—®é¢˜å¤±è´¥")


@app.get("/admin/cache/list")
async def get_cache_list(
    limit: int = 100,
    authorized: bool = Depends(verify_admin_token)
):
    """
    è·å–æ‰€æœ‰ç¼“å­˜åˆ—è¡¨ï¼ˆç®¡ç†å‘˜ï¼‰

    è¿”å›:
        ç¼“å­˜é—®é¢˜åˆ—è¡¨
    """
    if not semantic_cache or not semantic_cache.is_available():
        return {"cached_questions": []}

    try:
        cached_questions = semantic_cache.get_all_cached_questions(limit)
        return {"cached_questions": cached_questions}
        
    except Exception as e:
        logger.error(f"è·å–ç¼“å­˜åˆ—è¡¨æ—¶å‡ºé”™: {e}")
        raise HTTPException(status_code=500, detail="è·å–ç¼“å­˜åˆ—è¡¨å¤±è´¥")


@app.post("/admin/cache/add")
async def add_manual_cache(
    req: ManualCacheRequest,
    authorized: bool = Depends(verify_admin_token)
):
    """
    æ‰‹åŠ¨æ·»åŠ ç¼“å­˜ï¼ˆç®¡ç†å‘˜ï¼‰

    ç”¨äºæ·»åŠ ç²¾é€‰é—®ç­”å¯¹
    """
    if not semantic_cache or not semantic_cache.is_available():
        raise HTTPException(status_code=503, detail="ç¼“å­˜æœåŠ¡ä¸å¯ç”¨")

    try:
        # æ·»åŠ åˆ°é«˜ä¼˜å…ˆçº§ç¼“å­˜
        semantic_cache.set(
            req.question,
            req.answer,
            cache_type="manual",
            quality_score=req.quality_score,
            source_info=req.source_info  # ç®¡ç†å‘˜å¡«å†™çš„æºæ–‡ä»¶ä¿¡æ¯
        )
        
        cache_id = semantic_cache._compute_hash(req.question)
        logger.info(f"âœ… ç®¡ç†å‘˜æ‰‹åŠ¨æ·»åŠ ç¼“å­˜: {req.question[:50]}")
        
        return {
            "status": "success",
            "cache_id": cache_id,
            "message": "å·²æ·»åŠ åˆ°é«˜ä¼˜å…ˆçº§ç¼“å­˜"
        }
        
    except Exception as e:
        logger.error(f"æ‰‹åŠ¨æ·»åŠ ç¼“å­˜æ—¶å‡ºé”™: {e}")
        raise HTTPException(status_code=500, detail="æ·»åŠ ç¼“å­˜å¤±è´¥")


@app.delete("/admin/cache/clear")
async def clear_cache(
    req: ClearCacheRequest,
    authorized: bool = Depends(verify_admin_token)
):
    """
    æ¸…é™¤ç¼“å­˜ï¼ˆç®¡ç†å‘˜ï¼‰

    å‚æ•°:
        cache_types: è¦æ¸…é™¤çš„ç¼“å­˜ç±»å‹åˆ—è¡¨ï¼ˆNone è¡¨ç¤ºå…¨éƒ¨ï¼‰
        confirm: å¿…é¡»ä¸º true æ‰èƒ½æ‰§è¡Œ
    """
    if not req.confirm:
        raise HTTPException(status_code=400, detail="å¿…é¡»ç¡®è®¤æ¸…é™¤æ“ä½œ")

    if not semantic_cache or not semantic_cache.is_available():
        raise HTTPException(status_code=503, detail="ç¼“å­˜æœåŠ¡ä¸å¯ç”¨")

    try:
        deleted_count = semantic_cache.clear_cache(req.cache_types)
        
        cache_types_str = ", ".join(req.cache_types) if req.cache_types else "æ‰€æœ‰"
        logger.warning(f"ğŸ—‘ï¸ ç®¡ç†å‘˜æ¸…é™¤ç¼“å­˜: {cache_types_str} ({deleted_count} æ¡)")
        
        return {
            "status": "success",
            "deleted_count": deleted_count,
            "message": f"å·²æ¸…é™¤ {deleted_count} æ¡ç¼“å­˜"
        }
        
    except Exception as e:
        logger.error(f"æ¸…é™¤ç¼“å­˜æ—¶å‡ºé”™: {e}")
        raise HTTPException(status_code=500, detail="æ¸…é™¤ç¼“å­˜å¤±è´¥")


@app.delete("/admin/cache/{cache_id}")
async def delete_cache_item(
    cache_id: str,
    authorized: bool = Depends(verify_admin_token)
):
    """åˆ é™¤å•ä¸ªç¼“å­˜æ¡ç›®ï¼ˆç®¡ç†å‘˜ï¼‰"""
    if not semantic_cache or not semantic_cache.is_available():
        raise HTTPException(status_code=503, detail="ç¼“å­˜æœåŠ¡ä¸å¯ç”¨")

    try:
        semantic_cache._evict_cache(cache_id)
        logger.info(f"ğŸ—‘ï¸ ç®¡ç†å‘˜åˆ é™¤ç¼“å­˜: {cache_id}")
        
        return {
            "status": "success",
            "message": "å·²åˆ é™¤ç¼“å­˜æ¡ç›®"
        }
        
    except Exception as e:
        logger.error(f"åˆ é™¤ç¼“å­˜æ—¶å‡ºé”™: {e}")
        raise HTTPException(status_code=500, detail="åˆ é™¤ç¼“å­˜å¤±è´¥")


@app.get("/stats")
async def get_stats():
    """è·å–ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯"""
    return {
        "vector_db": {
            "document_count": vector_db.count(),
            "collection_name": config.CHROMA_COLLECTION_NAME
        },
        "parent_store": {
            "section_count": len(parent_store)
        },
        "config": {
            "retrieval_count": config.RETRIEVAL_COUNT,
            "rerank_top_k": config.RERANK_TOP_K,
            "vector_threshold": config.VECTOR_SEARCH_THRESHOLD,
            "rerank_threshold": config.RERANK_THRESHOLD
        }
    }


# ==================== é™æ€æ–‡ä»¶æŒ‚è½½ ====================
# å›¾ç‰‡ç›®å½•
if config.IMAGE_DIR.exists():
    app.mount(
        "/images",
        StaticFiles(directory=str(config.IMAGE_DIR)),
        name="images"
    )
    logger.info(f"âœ“ æŒ‚è½½å›¾ç‰‡ç›®å½•: {config.IMAGE_DIR}")

# å‰ç«¯é™æ€æ–‡ä»¶
if config.STATIC_DIR.exists():
    app.mount(
        "/",
        StaticFiles(directory=str(config.STATIC_DIR), html=True),
        name="static"
    )
    logger.info(f"âœ“ æŒ‚è½½é™æ€æ–‡ä»¶ç›®å½•: {config.STATIC_DIR}")


def main():
    """ä¸»å‡½æ•° - å¯åŠ¨æœåŠ¡å™¨"""
    import uvicorn

    logger.info("=" * 60)
    logger.info(f"å¯åŠ¨æœåŠ¡å™¨: http://{config.APP_HOST}:{config.APP_PORT}")
    logger.info(f"API æ–‡æ¡£: http://{config.APP_HOST}:{config.APP_PORT}/docs")
    logger.info("=" * 60)

    uvicorn.run(
        "main:app",
        host=config.APP_HOST,
        port=config.APP_PORT,
        reload=config.APP_RELOAD,
        log_level="info"
    )


if __name__ == "__main__":
    main()
