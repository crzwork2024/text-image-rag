{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09d82739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: 成功读取文件: book.md, 总字符数: 86910\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import uuid\n",
    "import logging\n",
    "import json\n",
    "import os\n",
    "\n",
    "# 1. 配置 Logging\n",
    "# 在 Jupyter 中，使用 stream=sys.stdout 确保日志输出在单元格下方\n",
    "import sys\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(levelname)s: %(message)s',\n",
    "    stream=sys.stdout, \n",
    "    force=True\n",
    ")\n",
    "\n",
    "# 2. 读取 book.md 文件\n",
    "file_path = 'book.md'\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    logging.error(f\"找不到文件: {file_path}，请确认文件名和路径是否正确。\")\n",
    "else:\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        book_content = f.read()\n",
    "    logging.info(f\"成功读取文件: {file_path}, 总字符数: {len(book_content)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd47bb14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: --- 调试开始：共检测到 79 个章节 ---\n",
      "INFO: 正在处理章节 [1]: # 5... (包含 1 段)\n",
      "INFO: 正在处理章节 [2]: # Requirements and Architecture for AI Pipelines... (包含 6 段)\n",
      "INFO: 正在处理章节 [3]: # Development pipelines... (包含 14 段)\n",
      "INFO: 正在处理章节 [4]: # Data store requirements... (包含 2 段)\n",
      "INFO: 正在处理章节 [5]: # Data volume and velocity... (包含 2 段)\n",
      "INFO: 正在处理章节 [6]: # Data formats and processing approaches... (包含 2 段)\n",
      "INFO: 正在处理章节 [7]: # Timeliness and technology selection... (包含 2 段)\n",
      "INFO: 正在处理章节 [8]: # Non-functional requirements and governance... (包含 2 段)\n",
      "INFO: 正在处理章节 [9]: # Support operations and specialized stores... (包含 5 段)\n",
      "INFO: 正在处理章节 [10]: # Algorithmic development components... (包含 2 段)\n",
      "INFO: 正在处理章节 [11]: # Data quality checks... (包含 3 段)\n",
      "INFO: 正在处理章节 [12]: # Data transforms... (包含 3 段)\n",
      "INFO: 正在处理章节 [13]: # Data summary... (包含 3 段)\n",
      "INFO: 正在处理章节 [14]: # Model building, tuning, and verification... (包含 6 段)\n",
      "INFO: 正在处理章节 [15]: # Configuration control... (包含 2 段)\n",
      "INFO: 正在处理章节 [16]: # Machine learning performance... (包含 2 段)\n",
      "INFO: 正在处理章节 [17]: # Computation infrastructure... (包含 2 段)\n",
      "INFO: 正在处理章节 [18]: # Scale processing... (包含 2 段)\n",
      "INFO: 正在处理章节 [19]: # Model tuning and verification... (包含 3 段)\n",
      "INFO: 正在处理章节 [20]: # Code committal and DevOps... (包含 3 段)\n",
      "INFO: 正在处理章节 [21]: # Production pipeline... (包含 2 段)\n",
      "INFO: 正在处理章节 [22]: # Data stores... (包含 5 段)\n",
      "INFO: 正在处理章节 [23]: # Data operations... (包含 3 段)\n",
      "INFO: 正在处理章节 [24]: # Data cleansing... (包含 3 段)\n",
      "INFO: 正在处理章节 [25]: # Data transformation... (包含 3 段)\n",
      "INFO: 正在处理章节 [26]: # Model execution... (包含 2 段)\n",
      "INFO: 正在处理章节 [27]: # Operational status monitoring... (包含 2 段)\n",
      "INFO: 正在处理章节 [28]: # Production Pipeline... (包含 7 段)\n",
      "INFO: 正在处理章节 [29]: # Model maintenance... (包含 2 段)\n",
      "INFO: 正在处理章节 [30]: # Results and end user stores... (包含 2 段)\n",
      "INFO: 正在处理章节 [31]: # Pipeline operations store... (包含 7 段)\n",
      "INFO: 正在处理章节 [32]: # Continuous development/integration... (包含 8 段)\n",
      "INFO: 正在处理章节 [33]: # Architecture patterns and tactics... (包含 6 段)\n",
      "INFO: 正在处理章节 [34]: # Non-functional requirements... (包含 2 段)\n",
      "INFO: 正在处理章节 [35]: # Reliability... (包含 2 段)\n",
      "INFO: 正在处理章节 [36]: # Maintainability... (包含 2 段)\n",
      "INFO: 正在处理章节 [37]: # Usability... (包含 2 段)\n",
      "INFO: 正在处理章节 [38]: # Summary... (包含 5 段)\n",
      "INFO: 正在处理章节 [39]: # Exercises... (包含 2 段)\n",
      "INFO: 正在处理章节 [40]: # References... (包含 2 段)\n",
      "INFO: 正在处理章节 [41]: # Unlock this book’s exclusive benefits now... (包含 3 段)\n",
      "INFO: 正在处理章节 [42]: # 6... (包含 1 段)\n",
      "INFO: 正在处理章节 [43]: # Design, Integration, and Testing... (包含 5 段)\n",
      "INFO: 正在处理章节 [44]: # Design fundamentals... (包含 2 段)\n",
      "INFO: 正在处理章节 [45]: # Requirements... (包含 2 段)\n",
      "INFO: 正在处理章节 [46]: # Performance requirements... (包含 3 段)\n",
      "INFO: 正在处理章节 [47]: # Non-functional requirements... (包含 3 段)\n",
      "INFO: 正在处理章节 [48]: # Security requirements... (包含 3 段)\n",
      "INFO: 正在处理章节 [49]: # Compliance requirements... (包含 3 段)\n",
      "INFO: 正在处理章节 [50]: # Actors and use cases... (包含 8 段)\n",
      "INFO: 正在处理章节 [51]: # System modes... (包含 10 段)\n",
      "INFO: 正在处理章节 [52]: # Block definition diagrams... (包含 3 段)\n",
      "INFO: 正在处理章节 [53]: # Data cleansing... (包含 3 段)\n",
      "INFO: 正在处理章节 [54]: # Data transformation... (包含 3 段)\n",
      "INFO: 正在处理章节 [55]: # Machine learning model... (包含 3 段)\n",
      "INFO: 正在处理章节 [56]: # Pipeline operations... (包含 3 段)\n",
      "INFO: 正在处理章节 [57]: # Results store... (包含 3 段)\n",
      "INFO: 正在处理章节 [58]: # System tactics and patterns... (包含 2 段)\n",
      "INFO: 正在处理章节 [59]: # Key attributes... (包含 2 段)\n",
      "INFO: 正在处理章节 [60]: # Maintainability tactics and patterns... (包含 4 段)\n",
      "INFO: 正在处理章节 [61]: # Availability tactics and patterns... (包含 3 段)\n",
      "INFO: 正在处理章节 [62]: # Essential patterns for AI systems... (包含 12 段)\n",
      "INFO: 正在处理章节 [63]: # Integration and testing... (包含 2 段)\n",
      "INFO: 正在处理章节 [64]: # Types of integrations... (包含 7 段)\n",
      "INFO: 正在处理章节 [65]: # Integration harness... (包含 6 段)\n",
      "INFO: 正在处理章节 [66]: # Testing types... (包含 6 段)\n",
      "INFO: 正在处理章节 [67]: # Requirements testing... (包含 5 段)\n",
      "INFO: 正在处理章节 [68]: # Use case and scenario testing... (包含 5 段)\n",
      "INFO: 正在处理章节 [69]: # Load testing... (包含 4 段)\n",
      "INFO: 正在处理章节 [70]: # Model prediction testing... (包含 5 段)\n",
      "INFO: 正在处理章节 [71]: # Data quality testing... (包含 4 段)\n",
      "INFO: 正在处理章节 [72]: # Error and fault recovery testing... (包含 4 段)\n",
      "INFO: 正在处理章节 [73]: # Compliance testing... (包含 5 段)\n",
      "INFO: 正在处理章节 [74]: # User interface testing... (包含 5 段)\n",
      "INFO: 正在处理章节 [75]: # Continuous development and integration... (包含 6 段)\n",
      "INFO: 正在处理章节 [76]: # Summary... (包含 9 段)\n",
      "INFO: 正在处理章节 [77]: # Exercises... (包含 3 段)\n",
      "INFO: 正在处理章节 [78]: # References... (包含 2 段)\n",
      "INFO: 正在处理章节 [79]: # Unlock this book’s exclusive benefits now... (包含 4 段)\n",
      "INFO: --- 调试完成：最终生成 223 个有效分块 ---\n"
     ]
    }
   ],
   "source": [
    "def debug_book_chunking(md_text):\n",
    "    # 按标题行切分章节 (匹配 #, ##, ### 等)\n",
    "    # 使用正向预查 (?=\\n#+ ) 保证标题本身被保留在 section 内容里\n",
    "    sections = re.split(r'\\n(?=#+ )', md_text)\n",
    "    \n",
    "    final_chunks = []\n",
    "    logging.info(f\"--- 调试开始：共检测到 {len(sections)} 个章节 ---\")\n",
    "\n",
    "    for i, section in enumerate(sections):\n",
    "        section = section.strip()\n",
    "        if not section: continue\n",
    "        \n",
    "        # Parent Context 就是这一章的所有原始文本\n",
    "        parent_context = section\n",
    "        \n",
    "        # 将章节内容按段落（双换行）切分\n",
    "        paragraphs = [p.strip() for p in section.split('\\n\\n') if p.strip()]\n",
    "        \n",
    "        # 记录当前章节的概况\n",
    "        title_line = section.split('\\n')[0][:50]\n",
    "        logging.info(f\"正在处理章节 [{i+1}]: {title_line}... (包含 {len(paragraphs)} 段)\")\n",
    "\n",
    "        for p_idx, para in enumerate(paragraphs):\n",
    "            # 过滤：如果这一段本身就是标题行，跳过（不作为 Child Embedding）\n",
    "            if para.startswith('#'):\n",
    "                continue\n",
    "            \n",
    "            # 长度检测：这是解决 413 报错的关键\n",
    "            char_len = len(para)\n",
    "            if char_len > 3000:\n",
    "                logging.warning(f\"  ⚠️ 发现超长段落! [第 {i+1} 章, 第 {p_idx} 段] 长度: {char_len} 字符\")\n",
    "                # 预览前100个字符定位问题\n",
    "                logging.warning(f\"     预览: {para[:100]}...\")\n",
    "            \n",
    "            final_chunks.append({\n",
    "                \"chunk_index\": len(final_chunks),\n",
    "                \"child_paragraph\": para,\n",
    "                \"metadata\": {\n",
    "                    \"parent_section\": parent_context # RAG召回后喂给LLM的完整上下文\n",
    "                }\n",
    "            })\n",
    "            \n",
    "    logging.info(f\"--- 调试完成：最终生成 {len(final_chunks)} 个有效分块 ---\")\n",
    "    return final_chunks\n",
    "\n",
    "# 执行调试\n",
    "if 'book_content' in locals():\n",
    "    all_chunks = debug_book_chunking(book_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bb5bf2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "【分块验证 - Index 0】\n",
      "Child (去 Embedding 的文本):\n",
      "Machine learning model development fundamentally differs from traditional software engineering in its experimental and iterative nature. While software engineers typically design systems based on well-defined specifications, data scientists must navigate the inherent uncertainties of data characteristics, feature relevance, and model behavior. This necessitates a systematic yet flexible approach to model creation, optimization, and validation that accommodates the unique challenges of AI development.\n",
      "------------------------------\n",
      "Parent Metadata (喂给 LLM 的上下文 - 截取前200字):\n",
      "# Requirements and Architecture for AI Pipelines\n",
      "\n",
      "Machine learning model development fundamentally differs from traditional software engineering in its experimental and iterative nature. While softwar...\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 查看前 2 个分块的详细结构\n",
    "if all_chunks:\n",
    "    test_idx = 0\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"【分块验证 - Index {test_idx}】\")\n",
    "    print(f\"Child (去 Embedding 的文本):\\n{all_chunks[test_idx]['child_paragraph']}\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Parent Metadata (喂给 LLM 的上下文 - 截取前200字):\\n{all_chunks[test_idx]['metadata']['parent_section'][:200]}...\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4eb2452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: ✅ 调试结果已保存至: c:\\Users\\RONGZHEN CHEN\\Desktop\\Projects\\multimodual-rag\\rag_project\\debug_chunks.json\n",
      "INFO: 您可以现在打开该文件查看 'child_paragraph' 与 'parent_section' 的映射关系。\n"
     ]
    }
   ],
   "source": [
    "output_filename = \"debug_chunks.json\"\n",
    "\n",
    "try:\n",
    "    with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        # indent=4 让 JSON 有层级缩进，方便肉眼查看\n",
    "        # ensure_ascii=False 保证中文字符正常显示而非 \\u 编码\n",
    "        json.dump(all_chunks, f, indent=4, ensure_ascii=False)\n",
    "    \n",
    "    logging.info(f\"✅ 调试结果已保存至: {os.path.abspath(output_filename)}\")\n",
    "    logging.info(f\"您可以现在打开该文件查看 'child_paragraph' 与 'parent_section' 的映射关系。\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"❌ 保存 JSON 失败: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db09792e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector items: 223\n",
      "Unique Parent sections: 79\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "import json\n",
    "import re\n",
    "import logging\n",
    "\n",
    "def get_hash(text):\n",
    "    \"\"\"Generates a short unique ID for a section of text.\"\"\"\n",
    "    return hashlib.md5(text.encode('utf-8')).hexdigest()\n",
    "\n",
    "def optimize_chunking_with_hashing(md_text):\n",
    "    sections = re.split(r'\\n(?=#+ )', md_text)\n",
    "    \n",
    "    vector_data = []      # For ChromaDB\n",
    "    parent_map = {}       # For the lookup file\n",
    "    \n",
    "    for section in sections:\n",
    "        section = section.strip()\n",
    "        if not section: continue\n",
    "        \n",
    "        # 1. Create the Hash for this section\n",
    "        section_hash = get_hash(section)\n",
    "        parent_map[section_hash] = section\n",
    "        \n",
    "        # 2. Split into paragraphs\n",
    "        paragraphs = [p.strip() for p in section.split('\\n\\n') if p.strip()]\n",
    "        \n",
    "        for para in paragraphs:\n",
    "            if para.startswith('#'): continue\n",
    "            \n",
    "            vector_data.append({\n",
    "                \"child_text\": para,\n",
    "                \"parent_hash\": section_hash  # Only store the hash here!\n",
    "            })\n",
    "            \n",
    "    return vector_data, parent_map\n",
    "\n",
    "# --- Run the optimization ---\n",
    "with open('book.md', 'r', encoding='utf-8') as f:\n",
    "    content = f.read()\n",
    "\n",
    "vector_list, doc_map = optimize_chunking_with_hashing(content)\n",
    "\n",
    "# Save the Vector Data (Small)\n",
    "with open('vector_ingest.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(vector_list, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "# Save the Parent Map (The \"Database\")\n",
    "with open('parent_store.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(doc_map, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"Vector items: {len(vector_list)}\")\n",
    "print(f\"Unique Parent sections: {len(doc_map)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ecb557",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
